{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Tp2_7506.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elemi10/7506-TP-Org-de-datos/blob/master/Tp2_7506.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrL_RUeZO7P0",
        "colab_type": "text"
      },
      "source": [
        "# Tp-2 Org de datos( FIUBA)\n",
        "\n",
        "\n",
        "    \n",
        "      \n",
        "      \n",
        "      \n",
        "***\n",
        "\n",
        "***\n",
        "### Importacion de librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywvX4yltPf1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORT FILES FROM DRIVE INTO GOOGLE-COLAB:\n",
        "\n",
        "#STEP-1: Import Libraries\n",
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXWPAkrDPogU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1JJnZjP9eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP-3: Get File from Drive using file-ID\n",
        "\n",
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'1RAGDjlzJ6spO5Sq8_x3UTIvxLhKAUBEt'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('train.csv') \n",
        "\n",
        "downloaded1 = drive.CreateFile({'id':'17pAgG9oJRK1bAFWRKkp96__zicG6yUmy'}) # replace the id with id of file you want to access\n",
        "downloaded1.GetContentFile('test.csv') \n",
        "\n",
        "downloaded2 = drive.CreateFile({'id':'1u8v51BT7FZggIRD-eo0dQno--0wlxIhA'}) # replace the id with id of file you want to access\n",
        "downloaded2.GetContentFile('sample_submission.csv') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x7YlhZuO7P2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "4cac48a8-7d8b-41e6-8bb4-be2ebb5f1c79"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F_KxoiU8sZB",
        "colab_type": "text"
      },
      "source": [
        "# Secci√≥n nueva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8yrHmRMO7QE",
        "colab_type": "text"
      },
      "source": [
        "### Archivos necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poifwBbpO7QI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "cdacc54f-3db9-4cc5-d763-6b58bcec4ea3"
      },
      "source": [
        "train=pd.read_csv(r\"train.csv\")\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8DK7J68O7QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "c7a3afe4-3d4a-42c9-fb8d-ad6355f355e0"
      },
      "source": [
        "test=pd.read_csv(r\"test.csv\")\n",
        "test.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>10861</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EARTHQUAKE SAFETY LOS ANGELES ¬â√õ√í SAFETY FASTE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3259</th>\n",
              "      <td>10865</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3260</th>\n",
              "      <td>10868</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>10874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>10875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id keyword location                                               text\n",
              "3258  10861     NaN      NaN  EARTHQUAKE SAFETY LOS ANGELES ¬â√õ√í SAFETY FASTE...\n",
              "3259  10865     NaN      NaN  Storm in RI worse than last hurricane. My city...\n",
              "3260  10868     NaN      NaN  Green Line derailment in Chicago http://t.co/U...\n",
              "3261  10874     NaN      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
              "3262  10875     NaN      NaN  #CityofCalgary has activated its Municipal Eme..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWi9v0_O7Qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9f08c34d-801e-4fe5-f36d-38cc1f1e56d6"
      },
      "source": [
        "sample_submission=pd.read_csv(r\"sample_submission.csv\")\n",
        "sample_submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target\n",
              "0   0       0\n",
              "1   2       0\n",
              "2   3       0\n",
              "3   9       0\n",
              "4  11       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZNM5WHLO7Qr",
        "colab_type": "text"
      },
      "source": [
        "## Generacion de algunas variables y nuevos DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuAiGp2SO7Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text=train.text\n",
        "train_target=train.target\n",
        "test_text=test.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEbWDZPGO7Q3",
        "colab_type": "text"
      },
      "source": [
        "##### Datasets que con nulos rellenados como 'none'\n",
        "****\n",
        "* Despues probar rellenando con 'nokeyword','nolocation'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj-nbqGAO7Q5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "e2734ac6-7491-4d6f-e5b9-5e176ec8dbd9"
      },
      "source": [
        "train_na=train.fillna(value='none')\n",
        "test_na=test.fillna(value='none')\n",
        "train_na['keyword+text']=train_na.text+' '+train_na.keyword\n",
        "test_na['keyword+text']=test_na.text+' '+test_na.keyword\n",
        "train_na['Caracteres']=train_na.text.str.len()\n",
        "test_na['Caracteres']=test_na.text.str.len()\n",
        "train_na.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>keyword+text</th>\n",
              "      <th>Caracteres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>10869</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>10870</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>1</td>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>10871</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>10872</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>10873</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... Caracteres\n",
              "7608  10869  ...         83\n",
              "7609  10870  ...        125\n",
              "7610  10871  ...         65\n",
              "7611  10872  ...        137\n",
              "7612  10873  ...         94\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zu-MY-wO7RD",
        "colab_type": "text"
      },
      "source": [
        "### Clasificadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSxpVUMSO7RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Establecemos a la regresion logistica como clasificador\n",
        "# Arbol como clasificador\n",
        "cls=LogisticRegression(random_state=0,solver='liblinear')\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "gbc = GradientBoostingClassifier(random_state=0)\n",
        "rfc=RandomForestClassifier(random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGqsQM02O7RJ",
        "colab_type": "text"
      },
      "source": [
        "# *MODELOS*\n",
        " \n",
        "  \n",
        "   \n",
        "    \n",
        "     \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdxaORxO7RO",
        "colab_type": "text"
      },
      "source": [
        "######   **Modelo 0**: Simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79p7sasOO7RQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "164ad80a-99c4-4e49-bc33-484708b84219"
      },
      "source": [
        "#Vectorizer, simple \n",
        "vectorizer=CountVectorizer()\n",
        "vectorizer.fit(train_text)\n",
        "X_train=vectorizer.transform(train_text)\n",
        "X_test=vectorizer.transform(test_text)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x21637 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 111497 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6N9OteO7RX",
        "colab_type": "text"
      },
      "source": [
        "###### *Modelo 1*: Agregando stopwords y eliminando caracteres tiles y ese tipo de cosas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FbAMTu3O7RY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "acc098c2-fcdc-4c32-9436-03cdad54b050"
      },
      "source": [
        "#Vectorizer1, agregando stopwords y distintos n_grams\n",
        "stop_words=stopwords.words('english')\n",
        "vectorizer1=CountVectorizer(strip_accents='ascii',stop_words=stop_words,ngram_range=(1,5),max_df=0.95)\n",
        "vectorizer1.fit(train_text)\n",
        "X_train1=vectorizer1.transform(train_text)\n",
        "X_test1=vectorizer1.transform(test_text)\n",
        "X_train1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x232480 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 344172 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3X5rmD3O7Rf",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 2: Uso de TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UauxkUhOO7Rh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be912122-3679-49c6-e6d7-d92d85317528"
      },
      "source": [
        "#Vectorizer2, usando TF-IDF\n",
        "vectorizer2=TfidfVectorizer()\n",
        "vectorizer2.fit(train_text)\n",
        "X_train2=vectorizer2.transform(train_text)\n",
        "X_test2=vectorizer2.transform(test_text)\n",
        "X_train2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x21637 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 111497 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdt5jPC9O7Ro",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 3: Uso de TF-IDF,sacando acentos y analizando varios n_grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R3vhrcyO7Rq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ab8f950b-faae-4289-aed3-73289f943f8d"
      },
      "source": [
        "# Vectorizer3, usando TF-iDF, sacando acentos y agreando n_grams\n",
        "max_n3=15\n",
        "vectorizer3=TfidfVectorizer(strip_accents='ascii',ngram_range=(1,max_n3),)\n",
        "vectorizer3.fit(train_text)\n",
        "X_train3=vectorizer3.transform(train_text)\n",
        "X_test3=vectorizer3.transform(test_text)\n",
        "X_train3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x769185 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1004105 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeRmWwc_O7Ry",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 4: Inclusion de Lemmatizador, NO incluye uso de TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjmOKMBzO7Rz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "72c8df3b-6a0a-4201-84ab-94f669875a05"
      },
      "source": [
        "# Vectorizer4\n",
        "# Creamos un lemmatizador \n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "vectorizer4=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer4.fit(train_text)\n",
        "X_train4=vectorizer4.transform(train_text)\n",
        "X_test4=vectorizer4.transform(test_text)\n",
        "X_train4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x22003 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 128166 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2w0mjiO7R5",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 5( mejores resultados):  \n",
        "- Uso de columna keyword + text\n",
        "- Lemmatizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqzscyKDO7R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text_y_keyword=train_na['keyword+text']\n",
        "test_text_y_keyword=test_na['keyword+text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTcDe22cO7SC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b3ef526f-c6fa-43ce-ce66-d177fae0e057"
      },
      "source": [
        "# Utilizo keyword en mi analisis\n",
        "vectorizer5=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer5.fit(train_text_y_keyword)\n",
        "X_train5=vectorizer5.transform(train_text_y_keyword)\n",
        "X_test5=vectorizer5.transform(test_text_y_keyword)\n",
        "X_train5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x22036 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 131385 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MgB1RI-O7SI",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 6: Utilizo keyword en mi analisis y TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd3H5dUTO7SK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "660f0f0b-e675-417a-bd4b-5b46d8015411"
      },
      "source": [
        "# Utilizo keyword en mi analisis y TF-IDF\n",
        "vectorizer6=TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer6.fit(train_text_y_keyword)\n",
        "X_train6=vectorizer6.transform(train_text_y_keyword)\n",
        "X_test6=vectorizer6.transform(test_text_y_keyword)\n",
        "X_train6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x22036 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 131385 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwID8NvzO7SO",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 7: Inclusion de columna location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx5M--hKO7SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agregamos columna location \n",
        "train_text_keyword_location=train_na.text+''+train_na.keyword+''+train_na.location\n",
        "test_text_keyword_location=test_na.text+''+test_na.keyword+''+test_na.location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPtdyA7ZO7SY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4f42b1cb-119a-46f0-87d5-73914a48e2b1"
      },
      "source": [
        "vectorizer7=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer7.fit(train_text_keyword_location)\n",
        "X_train7=vectorizer7.transform(train_text_keyword_location)\n",
        "X_test7=vectorizer7.transform(test_text_keyword_location)\n",
        "X_train7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7613x26489 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 138938 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajSBLrnzO7Se",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 8: Utilizamos funcion train_test_split.....\n",
        "- No tiene ningun tipo de mejora al modelo, es solo para probar, se puede utlizar para calcular el Score en lugar de hacer sub\n",
        "- Despues se generaliza para todo el set de entrenamiento y se predice set de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uoi67poO7Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_na['keyword+text']\n",
        "y=train_na['target']\n",
        "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3g3xgmzO7Sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "5c591db5-b256-4dc1-f351-874ab9551189"
      },
      "source": [
        "vectorizer8=CountVectorizer(strip_accents='ascii',tokenizer=LemmaTokenizer())\n",
        "vectorizer8.fit(X_train_)\n",
        "X_train8=vectorizer8.transform(X_train_)\n",
        "X_test8=vectorizer8.transform(X_test_)\n",
        "X_train8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6090x18904 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 105438 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nug83Y-CO7Sr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fe8f6f8-ee4a-45f5-ae7b-2a123afc1691"
      },
      "source": [
        "## Calculamos Score con este metodo, sin hacer cross validation\n",
        "cls.fit(X_train8,y_train_)\n",
        "prediction0=cls.predict(X_test8)\n",
        "print(f1_score(y_test_,prediction0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7649006622516556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9FYiV7O7UB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "938df7ee-5e3f-4cc5-ad3b-850c2dafca0a"
      },
      "source": [
        "RC=RidgeClassifier()\n",
        "RC.fit(X_train8,y_train_)\n",
        "RC_predict=RC.predict(X_test8)\n",
        "f1_score(RC_predict,y_test_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7297297297297298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqOQp9aeO7Sx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3ac52208-8f3f-4442-b777-fd9f48a85a44"
      },
      "source": [
        "### Progabmos el score con algunos de los clasificadores\n",
        "gbc.fit(X_train8,y_train_)\n",
        "cls.fit(X_train8,y_train_)\n",
        "prediction0=cls.predict(X_test8)\n",
        "prediction1=gbc.predict(X_test8)\n",
        "print('RL: {}'.format(f1_score(y_test_,prediction0)))\n",
        "print('GBC: {}'.format(f1_score(y_test_,prediction1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RL: 0.7649006622516556\n",
            "GBC: 0.6727941176470588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P87TFsSO7S6",
        "colab_type": "text"
      },
      "source": [
        "## Clasificador propio\n",
        "- Lemmatizador\n",
        "- Cuenta cantidad de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3tyCwppO7S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lemmatizer(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.l = WordNetLemmatizer()\n",
        "        \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, x):\n",
        "        x = map(lambda r:  ' '.join([self.l.lemmatize(i.lower()) for i in r.split()]), x)\n",
        "        x = np.array(list(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DeXZnJO7TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length_text(x):\n",
        "    return scaler.fit_transform(np.array(x.str.len()).reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xhBXA9O7TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm=Lemmatizer()\n",
        "vec=CountVectorizer()\n",
        "lr=LogisticRegression(random_state=0,solver='liblinear')\n",
        "clasificador=Pipeline([\n",
        "    ('features',FeatureUnion([\n",
        "        ('text',Pipeline([\n",
        "            ('lm',lm),\n",
        "            ('vec',vec)])),\n",
        "        ('lenght',Pipeline([\n",
        "            ('car',FunctionTransformer(length_text,validate=False))\n",
        "        ]))\n",
        "    ])),\n",
        "    ('lr',lr)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0PcawPNO7TR",
        "colab_type": "text"
      },
      "source": [
        "- *Cross validation del nuevo clasificador*\n",
        "- *El texto es text+keyword*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3LmFZMdO7TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_score(estimator=clasificador,X=train_text_y_keyword,y=train_na.target,cv=6,scoring='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ziyLM_wO7Tc",
        "colab_type": "text"
      },
      "source": [
        "* Calculamos f1 para el nuevo clasificador  (Sigue usando el df spliteado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r36updUhO7Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clasificador.fit(X_train_,y_train_)\n",
        "clas_predict=clasificador.predict(X_test_)\n",
        "f1_score(clas_predict,y_test_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yoikuSlO7Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ahora hacemos que utilice todo el dataset, asi tiene mayor corpus\n",
        "clasificador.fit(train_text_y_keyword,train_na.target)\n",
        "sample_submission['target']=clasificador.predict(test_text_y_keyword)\n",
        "sample_submission.to_csv('sub_9(class).csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGkLIZjrO7Tv",
        "colab_type": "text"
      },
      "source": [
        "* Generalizamos para todo el set de train, asi tiene mayor corpus\n",
        "* Hacemos submit con nuevo clasificador\n",
        "* (No tiene el mejor resultado en kaggle, seria bueno hacer un grid search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlbipRWAY_SQ",
        "colab_type": "text"
      },
      "source": [
        "**Modelo con DF nulos rellenados con otros nombres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCfDjzNY0zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "train_fill=train.fillna(value={'keyword':'nokeyword','location':'nolocation'})\n",
        "test_fill=test.fillna(value={'keyword':'nokeyword','location':'nolocation'})\n",
        "train_fill['text+keyword']=train_fill.text+' '+train_fill.keyword\n",
        "test_fill['text+keyword']=test_fill.text+' '+test_fill.keyword\n",
        "train_fill['Caracteres']=scaler.fit_transform(np.array(train_fill.text.str.len()).reshape(-1,1))\n",
        "test_fill['Caracteres']=scaler.transform(np.array(test_fill.text.str.len()).reshape(-1,1))\n",
        "train_fill.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3TO517uPiZ",
        "colab_type": "text"
      },
      "source": [
        "###  Generacion de nuevo DF para el train que incluye la cantidad de caracteres como variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OgS0X7fY1HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer9=TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
        "X_train9=vectorizer9.fit_transform(train_fill['text+keyword'])\n",
        "X_test9=vectorizer9.transform(test_fill['text+keyword'])\n",
        "X_train9\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF2Bbg4rY1Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XX_train=pd.concat([train_fill['Caracteres'].to_frame(),pd.DataFrame(X_train9.toarray())],axis=1)\n",
        "YY_train=train_fill.target\n",
        "XX_test=pd.concat([test_fill['Caracteres'].to_frame(),pd.DataFrame(X_test9.toarray())],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEXHM_qcY1i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls.fit(XX_train,YY_train)\n",
        "predict=cls.predict(XX_test)\n",
        "sample_submission['target']=predict\n",
        "from google.colab import files\n",
        "sample_submission.to_csv(\"sub_10.csv\",index=False)\n",
        "files.download('sub_10.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytF8YvZ8jDVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "XX_train_, XX_test_, YY_train_, YY_test_ = train_test_split(XX_train, YY_train, test_size=0.20, random_state=0)\n",
        "params={'C':np.logspace(-3,3,7),'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']}\n",
        "cls=LogisticRegression()\n",
        "grid=GridSearchCV(cls,params,cv=8,scoring='f1')\n",
        "grid.fit(XX_train_,YY_train_)\n",
        "print('parametros tuneados con grid search: {}'.format(grid.best_params_))\n",
        "print('f1: ',grid.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_biyNyuBosB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b7y3hx-ghoL",
        "colab_type": "text"
      },
      "source": [
        "# Redes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79aFIvAFDgrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Hacemos el test_train metodo\n",
        "XX_train_, XX_test_, YY_train_, YY_test_ = train_test_split(XX_train, YY_train, test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpD1s7Z971Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim=XX_train_.shape[1]\n",
        "modelito=Sequential()\n",
        "modelito.add(layers.Dense(5,input_dim=dim,activation='relu'))\n",
        "modelito.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TramcaQK8pOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelito.compile(loss='binary_crossentropy',\\\n",
        "                 optimizer='adam',\\\n",
        "                 metrics=['accuracy'])\n",
        "modelito.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrT0RLl1_gW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=modelito.fit(XX_train_,YY_train_,\\\n",
        "                    epochs=100,verbose=False,\\\n",
        "                    validation_data=(XX_test_,YY_test_),batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN72jrGZEMCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelito.evaluate(XX_test_,YY_test_,verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvohq0caHVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=modelito.predict(XX_test_)\n",
        "pred_bool=np.round(pred).astype(int)\n",
        "print(classification_report(YY_test_,pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB1lbBqw49kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    from keras import backend as K\n",
        "### Definimos metricas para generar f1\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeGw0BvO3_gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZAaUbuGz0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Generalizacion\n",
        "dim1=XX_train.shape[1]\n",
        "red1=Sequential()\n",
        "red1.add(layers.Dense(5,input_dim=dim,activation='relu'))\n",
        "red1.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjnBgKQTH6Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red1.compile(loss='binary_crossentropy',\\\n",
        "                 optimizer='adam',\\\n",
        "                 metrics=['accuracy'])\n",
        "red1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ5MIIBpIAgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets1=red1.fit(XX_train,YY_train,\\\n",
        "                    epochs=100,verbose=False,\\\n",
        "                    batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egwoNexqIhuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_red1=red1.predict(XX_test)\n",
        "pred_red1=np.round(pred_red1).astype(int)\n",
        "sample_submission['target']=pred_red1\n",
        "sample_submission.to_csv('sub11(RN).csv',index=False)\n",
        "from google.colab import files\n",
        "files.download('sub11(RN).csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-hfhOHy4k8o",
        "colab_type": "text"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTFG47lXIFVU",
        "colab_type": "text"
      },
      "source": [
        "DF para hacer Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFSpyAtxIEGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['text+keyword'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kZBdeDdpGG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJnkDARSa9vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sRoXk9t89kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token=Tokenizer(num_words=5000)\n",
        "token.fit_on_texts(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhX6mAYD9ib3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=token.texts_to_sequences(X_train)\n",
        "X_emb_test=token.texts_to_sequences(X_test)\n",
        "vocab_size=len(token.word_index)+1\n",
        "maxlen=100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_S0ggp-bhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=pad_sequences(X_emb_train,padding='post',maxlen=maxlen)\n",
        "X_emb_test=pad_sequences(X_emb_test,padding='post',maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jvyAuJO-kWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_emb_train[0,:].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpgW6I1m_kZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2=Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlQKVLqkAGFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dims=50\n",
        "model2.add(layers.Embedding(input_dim=vocab_size,input_length=100,output_dim=dims))\n",
        "model2.add(layers.GlobalMaxPool1D())\n",
        "model2.add(layers.Dense(10, activation='relu'))\n",
        "model2.add(layers.Dense(1, activation='sigmoid'))\n",
        "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69kmpgC8Ayzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_emb=model2.fit(X_emb_train,y_train,epochs=20,verbose=2,validation_data=(X_emb_test,y_test),\\\n",
        "                     batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNWp2IGNNDwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graf_10210=plot_history(tweets_emb)   #### 10 capas densas, 2 epochs y 10 de batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6cP1BATUvqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graf_52032=plot_history(tweets_emb)  ###5 capas densas, 20 epochs y 32 de batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIiuIjLxNJVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict(X_emb_test)\n",
        "pred_bool=np.round(pred).astype(int)\n",
        "print(classification_report(y_test,pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zft1eONEReoa",
        "colab_type": "text"
      },
      "source": [
        "Generalizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF0yiHRjNlOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token1=Tokenizer(num_words=5000)\n",
        "token1.fit_on_texts(train_fill['text+keyword'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKg0eH72dDF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=token1.texts_to_sequences(train_fill['text+keyword'])\n",
        "X_emb_test=token1.texts_to_sequences(test_fill['text+keyword'])\n",
        "vocab_size=len(token1.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzjZ1xSwdPyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=pad_sequences(X_emb_train,padding='post',maxlen=100)\n",
        "X_emb_test=pad_sequences(X_emb_test,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoneZp7NddFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub12=Sequential()\n",
        "dims=50\n",
        "sub12.add(layers.Embedding(input_dim=vocab_size,input_length=100,output_dim=dims))\n",
        "sub12.add(layers.GlobalMaxPool1D())\n",
        "sub12.add(layers.Dense(10, activation='relu'))\n",
        "sub12.add(layers.Dense(1, activation='sigmoid'))\n",
        "sub12.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "sub12.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1RNgF-QdzdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_emb=sub12.fit(X_emb_train,train_fill['target'],epochs=2,verbose=2,\\\n",
        "                     batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhcLQNBbeGc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_emb=sub12.predict(X_emb_test)\n",
        "pred_emb=np.round(pred_emb).astype(int)\n",
        "sample_submission['target']=pred_emb\n",
        "sample_submission.to_csv('sub12(EMB).csv',index=False)\n",
        "files.download('sub12(EMB).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEhZrgne7cF",
        "colab_type": "text"
      },
      "source": [
        "**Embedding pre-entrenado**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykbd_o09l8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['text+keyword'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe3OWhO2xtth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOl5hKkPfc3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  \n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomtWg_WnEaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(r'glove.6B.100d.txt',token1.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2dg22z68_Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(token1.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jZ0usNw-Lld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_gl= Sequential()\n",
        "model_gl.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=embedding_dim, \n",
        "                           trainable=True))\n",
        "model_gl.add(layers.GlobalMaxPool1D())\n",
        "model_gl.add(layers.Dense(10, activation='relu'))\n",
        "model_gl.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_gl.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_gl.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vj7qaHZ_rKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_gl=model_gl.fit(X_emb_train,y_train,epochs=5,verbose=2,validation_data=(X_emb_test,y_test),\\\n",
        "                     batch_size=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDXjT_iAGIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tweets_gl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgeUj6bN0Fw",
        "colab_type": "text"
      },
      "source": [
        "Generalizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSd2zh-DEDpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token1=Tokenizer(num_words=5000)\n",
        "token1.fit_on_texts(train_fill['text+keyword'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Cmvh2HN1RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train_1=token1.texts_to_sequences(train_fill['text+keyword'])\n",
        "X_emb_test_1=token1.texts_to_sequences(test_fill['text+keyword'])\n",
        "vocab_size=len(token1.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmf3FhtNOcSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train_1=pad_sequences(X_emb_train_1,padding='post',maxlen=100)\n",
        "X_emb_test_1=pad_sequences(X_emb_test_1,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8IikAi2Oif2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_1 = create_embedding_matrix(r'glove.6B.100d.txt',token1.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9qs4QwOuFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(token1.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urRd5t1BOyjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub13= Sequential()\n",
        "sub13.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=50, \n",
        "                           trainable=True))\n",
        "sub13.add(layers.GlobalMaxPool1D())\n",
        "sub13.add(layers.Dense(10, activation='relu'))\n",
        "sub13.add(layers.Dense(1, activation='sigmoid'))\n",
        "sub13.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub13.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iPtKwrEO_sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_gl=sub13.fit(X_emb_train,train_fill['target'],epochs=5,verbose=2,batch_size=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2U2-MyjPgqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_gl=sub13.predict(X_emb_test)\n",
        "pred_gl=np.round(pred_gl).astype(int)\n",
        "sample_submission['target']=pred_gl\n",
        "sample_submission.to_csv('sub13(GLOVE).csv',index=False)\n",
        "files.download('sub13(GLOVE).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1JbunlYJ19G",
        "colab_type": "text"
      },
      "source": [
        "# CONV1d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3MMbZbVJ7j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim=100\n",
        "conv=Sequential()\n",
        "conv.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix], trainable=True))\n",
        "conv.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "conv.add(layers.GlobalMaxPooling1D())\n",
        "conv.add(layers.Dense(10, activation='relu'))\n",
        "conv.add(layers.Dense(1, activation='sigmoid'))\n",
        "conv.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "conv.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TS8SnKI6aH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tw_conv=conv.fit(X_emb_train,y_train,epochs=10,verbose=2,\n",
        "                 validation_data=(X_emb_test,y_test),batch_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GQCz3-c6yEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tw_conv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgOfUuVikBJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def create_model(init_mode='he_uniform'):\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix], trainable=False))\n",
        "    model.add(layers.Conv1D(num_filters,kernel_size, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXh5zV54nQbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
        "                           batch_size=batch_size, verbose=2)\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "\n",
        "param_grid = dict(init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_emb_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJpRLIwMyWin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEEViqxm3wFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def create_model_1(num_filters,kernel_size,vocab_size,embedding_dim,maxlen,init_mode='he_uniform'):\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(layers.Embedding(vocab_size,emb_dim,input_length=maxlen,weights=[embedding_matrix], trainable=True))\n",
        "    model.add(layers.Conv1D(num_filters,kernel_size, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TZ5Iz6y2k3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tuneo de hiperparametros\n",
        "%%time\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "param_grid_1= dict(num_filters=[32, 64, 128],kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen],epochs=[10,20],\n",
        "                  batch_size=[10,32,64])\n",
        "model_1 = KerasClassifier(build_fn=create_model_1, verbose=2)\n",
        "grid_1= RandomizedSearchCV(estimator=model_1, param_distributions=param_grid_1,\n",
        "                           cv=4, verbose=2, n_iter=5,n_jobs=-1,scoring='f1')\n",
        "grid_result_1 = grid_1.fit(X_emb_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh-oXIijplBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parametros=grid_result_1.best_params_\n",
        "parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAcBO7OxrT-j",
        "colab_type": "text"
      },
      "source": [
        "**Generalizacion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3WWIccDrXpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_14=Sequential()\n",
        "sub_14.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix_1], trainable=True))\n",
        "sub_14.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_14.add(layers.GlobalMaxPooling1D())\n",
        "sub_14.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_14.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_14.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_14.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgqesInlstMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub14=sub_14.fit(X_emb_train_1,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7zfS-4RvoYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_14=sub_14.predict(X_emb_test_1)\n",
        "pred_sub_14=np.round(pred_sub_14).astype(int)\n",
        "sample_submission['target']=pred_sub_14\n",
        "sample_submission.to_csv('sub14(GLOVE_tun).csv',index=False)\n",
        "files.download('sub14(GLOVE_tun).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-CNyIHByRe",
        "colab_type": "text"
      },
      "source": [
        "# Trabajo con dataset \"limpio\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhyatYcDJuRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleanr = re.compile('https?://\\S+|www\\.\\S+')\n",
        "p=[]\n",
        "for i in range(len(train_fill)):\n",
        "    p.append(re.sub(cleanr, '', train_fill['text+keyword'].loc[i]))\n",
        "train_fill['t&k_sin_url']=p\n",
        "p1=[]\n",
        "for i in range(len(test_fill)):\n",
        "    p1.append(re.sub(cleanr, '', test_fill['text+keyword'].loc[i]))\n",
        "test_fill['t&k_sin_url']=p1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D04EYc1oB3xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "st = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "train_fill['t&k_sin_url']=train_fill['t&k_sin_url'].apply(lambda x: clean_text(x))\n",
        "test_fill['t&k_sin_url']=test_fill['t&k_sin_url'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcUNrj5rSukx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_fill['t&k_sin_url']\n",
        "y=np.asarray(train_fill.target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ne4OPZDBP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=[]\n",
        "x=train_fill['t&k_sin_url']\n",
        "for i in range(len(x)):\n",
        "    a.append(TextBlob(train_fill.loc[i,'t&k_sin_url']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1-gaCWyHWE-",
        "colab": {}
      },
      "source": [
        "token2=Tokenizer(num_words=5000)\n",
        "token2.fit_on_texts(train_fill['t&k_sin_url'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lC5aQs9MHWFb",
        "colab": {}
      },
      "source": [
        "X_emb_train_2=token2.texts_to_sequences(train_fill['t&k_sin_url'])\n",
        "X_emb_test_2=token2.texts_to_sequences(test_fill['t&k_sin_url'])\n",
        "vocab_size_2=len(token2.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZSTtoppHWFq",
        "colab": {}
      },
      "source": [
        "X_emb_train_2=pad_sequences(X_emb_train_2,padding='post',maxlen=100)\n",
        "X_emb_test_2=pad_sequences(X_emb_test_2,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6ufjafPHWF3",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_2 = create_embedding_matrix(r'glove.6B.100d.txt',token2.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yWDgQ93eHWGF",
        "colab": {}
      },
      "source": [
        "vocab_size_2=len(token2.word_index)+1\n",
        "nonzero_elements_2 = np.count_nonzero(np.count_nonzero(embedding_matrix_2, axis=1))\n",
        "nonzero_elements_2/ vocab_size_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ALraQP31KEuk",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_15=Sequential()\n",
        "sub_15.add(layers.Embedding(vocab_size_2,embedding_dim,input_length=100,weights=[embedding_matrix_2], trainable=False))\n",
        "sub_15.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_15.add(layers.GlobalMaxPooling1D())\n",
        "sub_15.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_15.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_15.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_15.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pfYYhgwtKEvA",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub15=sub_15.fit(X_emb_train_2,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUgncKCK_Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_15=sub_15.predict(X_emb_test_2)\n",
        "pred_sub_15=np.round(pred_sub_15).astype(int)\n",
        "sample_submission['target']=pred_sub_15\n",
        "sample_submission.to_csv('sub15(GLOVE_tun(t=false)).csv',index=False)\n",
        "files.download('sub15(GLOVE_tun(t=false)).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxF6N01PMRBc",
        "colab_type": "text"
      },
      "source": [
        "Mismo modelo pero con trainable=true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyK7fBIkMP4j",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_16=Sequential()\n",
        "sub_16.add(layers.Embedding(vocab_size_2,embedding_dim,input_length=100,weights=[embedding_matrix_2], trainable=True))\n",
        "sub_16.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_16.add(layers.GlobalMaxPooling1D())\n",
        "sub_16.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_16.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_16.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_16.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PGlcsFuwMP47",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub16=sub_16.fit(X_emb_train_2,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f8jc0qQ5MP5O",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_16=sub_16.predict(X_emb_test_2)\n",
        "pred_sub_16=np.round(pred_sub_16).astype(int)\n",
        "sample_submission['target']=pred_sub_16\n",
        "sample_submission.to_csv('sub16(GLOVE_tun(t=true)).csv',index=False)\n",
        "files.download('sub16(GLOVE_tun(t=true)).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZDas6LMvoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['t&k_sin_url'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mRT_ToUju-Ui",
        "colab": {}
      },
      "source": [
        "token3=Tokenizer(num_words=10000)\n",
        "token3.fit_on_texts(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xi5JCHGOu-VM",
        "colab": {}
      },
      "source": [
        "X_emb_train_3=token3.texts_to_sequences(X)\n",
        "X_emb_test_3=token3.texts_to_sequences(test_fill['t&k_sin_url'])\n",
        "vocab_size_3=len(token3.word_index)+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L4imDOtku-Vq",
        "colab": {}
      },
      "source": [
        "maxlen=32\n",
        "X_emb_train_3=pad_sequences(X_emb_train_3,padding='post',maxlen=maxlen)\n",
        "X_emb_test_3=pad_sequences(X_emb_test_3,padding='post',maxlen=maxlen)\n",
        "indices = np.arange(X_emb_train_3.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X_emb_train_3= X_emb_train_3[indices]\n",
        "y=y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkihCitUOJyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_test_3[700]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcedHfs3u-XZ",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "dims=50\n",
        "modelito=Sequential()\n",
        "modelito.add(layers.Embedding(input_dim=vocab_size_3,input_length=maxlen,output_dim=dims))\n",
        "modelito.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "modelito.add(layers.GlobalMaxPooling1D())\n",
        "modelito.add(layers.Dense(32,activation='relu',kernel_initializer=init_mode))\n",
        "modelito.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "modelito.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "modelito.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "egPGOf7su-Xz",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub_modelito=modelito.fit(X_emb_train_3,y,epochs=10,verbose=2,batch_size=10,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_aNR2vScM6jB",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_3 = create_embedding_matrix(r'glove.6B.100d.txt',token3.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NymxGp7jM6jN",
        "colab": {}
      },
      "source": [
        "vocab_size_3=len(token3.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix_3, axis=1))\n",
        "nonzero_elements / vocab_size_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5fY8dzCTEQ7",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size_3,output_dim=25, input_length=maxlen))\n",
        "model.add(layers.Conv1D(32,kernel_size=5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D())\n",
        "model.add(layers.Conv1D(32, kernel_size=5,activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR5YTLDxaTSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc',f1_m])\n",
        "history = model.fit(X_emb_train_3,y,epochs=50,batch_size=10,validation_split=0.2,verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNoyFi7wchtL",
        "colab_type": "text"
      },
      "source": [
        "Modelo mas basico, usando **BOW** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZHxW8DvcgFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe=Pipeline([('vec',CountVectorizer()),\n",
        "               ('cls',LogisticRegression())])\n",
        "modelo=pipe.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roCF9QaDy0Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediccion=model_selection.cross_val_score(modelo,X,y,cv=6,scoring='f1')\n",
        "prediccion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQUVR-BLd0Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                 ('tfidf', TfidfTransformer()),\n",
        "                 ('model', DecisionTreeClassifier(criterion= 'entropy', max_depth = 20, splitter='best', random_state=0))])\n",
        "modelo= pipe.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMSUJ1MOfc3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediccion=model_selection.cross_val_score(modelo,X,y,cv=6,scoring='f1')\n",
        "prediccion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4s6PSdWhRRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer5=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer5.fit(X)\n",
        "X_train5=vectorizer5.transform(X)\n",
        "X_test5=vectorizer5.transform(test_fill['t&k_sin_url'])\n",
        "X_train5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ZsByFgjlDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=xgb.XGBClassifier(random_state=1)\n",
        "model.fit(X_train5,y)\n",
        "model_selection.cross_val_score(model,X_train5,y,scoring='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkvxOyyLkAnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "sample_submission['target']=pred\n",
        "sample_submission.to_csv('sub17(BOW_limpio).csv',index=False)\n",
        "files.download('sub17(BOW_limpio).csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}