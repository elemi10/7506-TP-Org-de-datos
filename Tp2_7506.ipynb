{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Tp2_7506.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elemi10/7506-TP-Org-de-datos/blob/master/Tp2_7506.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrL_RUeZO7P0",
        "colab_type": "text"
      },
      "source": [
        "# Tp-2 Org de datos( FIUBA)\n",
        "\n",
        "\n",
        "    \n",
        "      \n",
        "      \n",
        "      \n",
        "***\n",
        "\n",
        "***\n",
        "### Importacion de librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywvX4yltPf1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORT FILES FROM DRIVE INTO GOOGLE-COLAB:\n",
        "\n",
        "#STEP-1: Import Libraries\n",
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXWPAkrDPogU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "46a85b33-fd05-47a9-ef3e-84c756094b66"
      },
      "source": [
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-14b33244e28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#STEP-2: Autheticate E-Mail ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m       \u001b[0m_gcloud_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0m_install_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mcolab_tpu_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36m_gcloud_login\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mgcloud_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'strip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1JJnZjP9eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP-3: Get File from Drive using file-ID\n",
        "\n",
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'1RAGDjlzJ6spO5Sq8_x3UTIvxLhKAUBEt'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('train.csv') \n",
        "\n",
        "downloaded1 = drive.CreateFile({'id':'17pAgG9oJRK1bAFWRKkp96__zicG6yUmy'}) # replace the id with id of file you want to access\n",
        "downloaded1.GetContentFile('test.csv') \n",
        "\n",
        "downloaded2 = drive.CreateFile({'id':'1u8v51BT7FZggIRD-eo0dQno--0wlxIhA'}) # replace the id with id of file you want to access\n",
        "downloaded2.GetContentFile('sample_submission.csv') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x7YlhZuO7P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8yrHmRMO7QE",
        "colab_type": "text"
      },
      "source": [
        "### Archivos necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poifwBbpO7QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv(r\"train.csv\")\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8DK7J68O7QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=pd.read_csv(r\"test.csv\")\n",
        "test.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWi9v0_O7Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission=pd.read_csv(r\"sample_submission.csv\")\n",
        "sample_submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9o_RSZOIE-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZNM5WHLO7Qr",
        "colab_type": "text"
      },
      "source": [
        "## Generacion de algunas variables y nuevos DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuAiGp2SO7Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text=train.text\n",
        "train_target=train.target\n",
        "test_text=test.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEbWDZPGO7Q3",
        "colab_type": "text"
      },
      "source": [
        "##### Datasets que con nulos rellenados como 'none'\n",
        "****\n",
        "* Despues probar rellenando con 'nokeyword','nolocation'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj-nbqGAO7Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_na=train.fillna(value='none')\n",
        "test_na=test.fillna(value='none')\n",
        "train_na['keyword+text']=train_na.text+' '+train_na.keyword\n",
        "test_na['keyword+text']=test_na.text+' '+test_na.keyword\n",
        "train_na['Caracteres']=train_na.text.str.len()\n",
        "test_na['Caracteres']=test_na.text.str.len()\n",
        "train_na.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zu-MY-wO7RD",
        "colab_type": "text"
      },
      "source": [
        "### Clasificadores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSxpVUMSO7RE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Establecemos a la regresion logistica como clasificador\n",
        "# Arbol como clasificador\n",
        "cls=LogisticRegression(random_state=0,solver='liblinear')\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "gbc = GradientBoostingClassifier(random_state=0)\n",
        "rfc=RandomForestClassifier(random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGqsQM02O7RJ",
        "colab_type": "text"
      },
      "source": [
        "# *MODELOS*\n",
        " \n",
        "  \n",
        "   \n",
        "    \n",
        "     \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ZtSHN9beB_",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXRNZx1alA94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verifico que no haya instancias nulas o filas completas nulas\n",
        "df_train=df_train.dropna(how=\"all\")\n",
        "\n",
        "df_test=df_test.dropna(how=\"all\")\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOvXdYdllC3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizo el dataframe los registros NaN con texto\n",
        "df_train=df_train.fillna({'keyword': 'sin keyword',\\\n",
        "                   'location': 'sin location'})\n",
        "df_test=df_test.fillna({'keyword': 'sin keyword',\\\n",
        "                   'location': 'sin location'})\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0LlgXdAlHEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eliminamos los tweets duplicados considerando que su cantidad no tiene impacto en el análisis.\n",
        "df_train=df_train.drop_duplicates('text')\n",
        "\n",
        "df_test=df_test.drop_duplicates('text')\n",
        "\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9-OneNlV2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verificamos la existencia de valores nulos\n",
        "df_train.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOZqtrYrFgrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bhMB0YIwfNw",
        "colab_type": "text"
      },
      "source": [
        "### **Análisis & tokenización**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8_xHK_VwlSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función de tokenización\n",
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokens = []\n",
        "    for token in sentence.split():\n",
        "        new_token = []\n",
        "        for character in token:\n",
        "            if character not in punctuation:\n",
        "                new_token.append(character.lower())\n",
        "        if new_token:\n",
        "            tokens.append(\"\".join(new_token))\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArzzacYN81ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creo la serie 'text_vector'\n",
        "df_train['text_vector']=df_train['text'].apply(tokenize)\n",
        "df_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPmUdAqoFrMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['text_vector']=df_test['text'].apply(tokenize)\n",
        "df_test.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaBYYZ7T8_FC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verifico integridad de DF\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca-WaybyF0Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E8x1l769CIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descargo e importo lista de stopwords para filtrar\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNiZDXDG9vDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importo stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bT2hiea9ySY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Asigno en la variable 'stop' la categoria de stopword por la que voy a realizar el filtro\n",
        "stop=stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxbeEFDF-ewr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filtro(text_vector):\n",
        "  text_vector_filtrado = []\n",
        "  for palabra in text_vector:\n",
        "    if palabra not in stop:\n",
        "      text_vector_filtrado.append(palabra)\n",
        "  return(text_vector_filtrado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Bb1hKl-g5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokens filtrados por stopwords en inglés\n",
        "df_train['text_vector_filtrado']=df_train['text_vector'].apply(filtro)\n",
        "df_test['text_vector_filtrado']=df_test['text_vector'].apply(filtro)\n",
        "df_train['text_vector_filtrado'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3zoxDR8F-av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['text_vector_filtrado'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzMWe3qqILvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Convierto mis vectores filtrados en la columna text (los transformo en una serie)\n",
        "df_test['text']=df_test['text_vector_filtrado'].apply(lambda x: ' '.join(x) )\n",
        "df_train['text']=df_train['text_vector_filtrado'].apply(lambda x: ' '.join(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTu988Jx-npG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Asigno en una nueva columna el largo del vector 'text_vector'\n",
        "df_train['elem_vector']=df_train['text_vector'].str.len()\n",
        "df_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c87bJqvu-g4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cuento los elementos del vector filtrado 'text_vector_filtrado'\n",
        "df_train['elem_vector_filtrado']=df_train['text_vector_filtrado'].str.len()\n",
        "df_train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc78PyD7-gvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verifico estructura del DF\n",
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7syMKRb_wVt",
        "colab_type": "text"
      },
      "source": [
        "### Preproceso y categorización de la variable ***Keyword***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fauKBQRp_p0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Limpiamos\" la serie de caracteres especiales\n",
        "df_train['keyword']=df_train['keyword'].str.replace('%20','_')\n",
        "\n",
        "# Se lo aplicamos al set de test tambien\n",
        "df_test['keyword']=df_test['keyword'].str.replace('%20','_')\n",
        "\n",
        "# Verificamos actualización\n",
        "df_train['keyword'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d5T6PLZAFRi",
        "colab_type": "text"
      },
      "source": [
        "### Generación de **diccionario**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-UOXjE7-gtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Listo para generar un diccionario y traducir la serie 'keyword'\n",
        "keyword_list=df_train['keyword'].unique().tolist()\n",
        "print(keyword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Fw1BZi-gfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 Genero lista en espaniol\n",
        "keyword_list_esp=['sin_keyword','en llamas','accidente','replica','accidente_avion','ambulancia','aniquilado','aniquilacion','apocalipse','armageddon','ejercito','incendio_intencional','piromano','ataque','atacado','avalancha','combate/batalla/lucha','bioterror','bioterrorismo','resplandecer/arder','resplandeciente/ardiente','hemorragia','exploto','plaga','ventisca','sangre','sangriento','estallido','bolsa_de_cadaver','embolsado_de_cadaver','bolsas_de_cadaver','bomba','bombardeado','bombardeo','colapso_de_puente','incendio_de_edificios','edificios_en_llamas','quemado','quemando/incendiando','incendio_edificios','incendio_de_matorrales/arbustos','bajas/muertes/perdidas','baja/muerte/perdida','catastrofe','catastrofico','emergencia_quimica','caida_acantilado','colapso','colapsado','colisionar','colisiono','colision','choque','estrellado','aplastamiento','aplastado','toque_de_queda','ciclon','da¤o','peligro','muerto','muerte','muertes','escombros','diluvio','inundado','demoler','demolido','demolicion','descarrilar','descarrilado','descarrilamiento','desolado','desolacion','destruir','destruido','destruccion','detonado','detonacion','devastado','devastacion','desastre','dislacado/desplazado','sequia','ahogar','ahogado','ahogamiento/ahogandose','tormenta_polvo','terremoto','electrocutar','electrocutado','emergencia','plan_de_emergencia','servicio_de_emergencia','engullido','epicentro','evacuar','evacuado','evacuacion','explotar','exploto/detonado/estallado','explosion','testigo_ocular','hambruna','fatal','fatalidades','fatalidades','miedo','fuego','camion_bombero','primeros_en_responder','llamas','aplastado','inundacion','inundaciones','inundaciones','incendio_forestal','incendio_forestales','granizo','tormenta_granizo','da¤o','peligro','peligroso','ola_de_calor','infierno','secuestro','secuestrador','secuestro','rehen','rehenes','huracan','herido/lesionado','heridas/lesiones','herida/lesion','inundado','inundacion','deslisamiento_tierra','lava','rayo','golpe_fuerte/estampido_fuerte','asesinato_en_serie','asesino_en_serie','masacre','caos','fusion','militar','alud_de_lodo','desastre_natural','desastre_nuclear','reactor_nuclear','obliterar/destruir/eliminar','borrado/destruido/eliminado','obliteracion/destruccion/eliminacion','derrame_de_petroleo','brote','pandemonio/confusion/caos','panico','tener_panico','polic¡a','cuarentena','en_cuarentena','emergencia_radiacion','tormenta','arrasado','refugiados','rescate','rescatado','rescatadores','disturbio','disturbios','escombros','ruina','tormenta_de_arena','grito','gritando','gritos','sismico','sumidero/pozo','hundiendose','sirena','sirenas','humo','tormenta_de_nieve','tormenta','camilla','fallo_estructural','bomba_suicida','suicida','bombardeo_suicida','hundido','sobrevivir','sobrevivio','sobreviviente','terrorismo','terrorista','amenaza','trueno','tormenta_electrica','tornado','tragedia','atrapado','trauma','traumatizado','problema','tsunami','tornado/torbellino','tifon','transtorno','tormenta_violenta','volcan','zona_de_guerra','arma','armas','torbellino','incendios_forestales','incendio_forestal','tormenta_de_viento','moretoneado','moretones','naufragio','restos','naufrago']\n",
        "print(keyword_list_esp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNCgM0ujAbQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 Genero el diccionario eng-esp de la serie 'Keyword'\n",
        "keyword_dicc={}\n",
        "y=0\n",
        "for x in keyword_list:\n",
        "  keyword_dicc[x]=keyword_list_esp[y]\n",
        "  y=y+1\n",
        "  #print(keyword_dicc)\n",
        "\n",
        "print(keyword_dicc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy710P3cAcm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mapeo los valores de las key del diccionario keyword_dicc\n",
        "df_train['keyword_esp']=df_train['keyword'].map(keyword_dicc)\n",
        "\n",
        "df_test['keyword_esp']=df_test['keyword'].map(keyword_dicc)\n",
        "\n",
        "df_train.head(2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCZdMxMhPysZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4eq5g2DAmer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Verifico el top 20 de las categorias mas frecuentes para la serie keyword \n",
        "df_train['keyword_esp'].value_counts().sort_values(ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qyAZGfP7xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['keyword_esp'].value_counts().sort_values(ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0oOKHEmChGy",
        "colab_type": "text"
      },
      "source": [
        "### → **Scikit Learn**: Ejecución de **One Hot Encoding** para clasificar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1-A_g81CfzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dummifico las categorías\n",
        "df_train_dummies=pd.get_dummies(df_train.keyword)\n",
        "df_train_dummies.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqodSPjoXC50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_dummies=pd.get_dummies(df_test.keyword)\n",
        "df_test_dummies.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mCZu5xtCXs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hago un join con df_train y df_train_dummies\n",
        "df_train_merged = pd.concat([df_train,df_train_dummies],axis='columns')\n",
        "df_train_merged.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmVF_V29XLZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_merged = pd.concat([df_test,df_test_dummies],axis='columns')\n",
        "df_test_merged.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2OhxtVCXnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Elimino las columnas 'keyword_esp' y 'sin keyword'\n",
        "df_train_merged_final=df_train_merged.drop(['keyword_esp','sin keyword'],axis='columns')\n",
        "df_train_merged_final.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INWp8lHYXXkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_merged_final=df_test_merged.drop(['keyword_esp','sin keyword'],axis='columns')\n",
        "df_test_merged_final.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ldMLTArQXJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exporto a .csv para verificar estructura\n",
        "df_train_merged_final.to_csv('set_train_columna_texto_vector_filtrado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QrSTbyGXir5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_merged_final.to_csv('set_test_columna_texto_vector_filtrado.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JASxLrDK9nP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_train_merged_final['text_vector_filtrado'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAkQNxJbX1Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_test_merged_final['text_vector_filtrado'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eifuTrzTXWEM",
        "colab_type": "text"
      },
      "source": [
        "Vectorizo con el feature **CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8WrI8pvzLTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defino los documentos/vector (para fragmentar la cadena de palabras)\n",
        "documents=(df_train_merged_final['text'])\n",
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOiNTmDG4YP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se ignorara todas las palabras que se encuentren en la lista stop_words\n",
        "vectorizer=CountVectorizer(stop_words='english')\n",
        "#Ajustamos el conjunto de datos del documento al objeto countvectorizer\n",
        "vectors=vectorizer.fit_transform(documents)\n",
        "# conseguimos la lista de palabras que han sido clasificadas\n",
        "names=vectorizer.get_feature_names()\n",
        "names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awhB7kgI5Ffq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creamos una matriz. El valor correspondiente a la (fila, columna) sera la frecuencia de esa palabra\n",
        "doc_array=vectorizer.transform(documents).toarray()\n",
        "doc_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwN3bxYAWJew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# se verifica la cantidad de elementos de la lista\n",
        "len(doc_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD5VOUkr58bW",
        "colab_type": "text"
      },
      "source": [
        "### Implementación de **BoW** O **Bag of Word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_-10tzc5Y7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convertimos la matriz en una estructura de datos\n",
        "frecuency_matrix=pd.DataFrame(data=doc_array, columns=names)\n",
        "frecuency_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfGzZKq36MDN",
        "colab_type": "text"
      },
      "source": [
        "### Set de datos de **Pruebas** y de **Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBYTNqQQ6loW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Divido los conjuntos de entrenamiento y test\n",
        "X_train, X_test, y_train, y_test= train_test_split(df_train_merged_final['text'],\\\n",
        "                                                   df_train_merged_final['target'], random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scv5P2MC6lln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Cant total de filas en el set de datos: {}'.format(df_train_merged_final.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hYscw0m6ljO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Cant de filas en el set de datos de entrenamiento: {}'.format(X_train.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWlbGt2B6lcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Cant de filas en el set de datos de pruebas: {}'.format(X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Wytcbt96Cv",
        "colab_type": "text"
      },
      "source": [
        "### Ejecución de BoW para procesar los datos de pruebas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STrELXAU964n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iniciamos el metodo countvectorizer otra vez\n",
        "count_vector=CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvGFgYu597m4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajustamos el set de entrenamiento\n",
        "training_data=count_vector.fit_transform(X_train)\n",
        "training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrSj8-SI97b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ajustamos el set de pruebas\n",
        "testing_data=count_vector.transform(X_test)\n",
        "testing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qyUmBK0_Ska",
        "colab_type": "text"
      },
      "source": [
        "### Implementación de Naives Bayes Multinomial para clasificar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW6VYe9f_ZOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iniciamos los metodos y ajustamos los datos en el clasificador\n",
        "naives_bayes=MultinomialNB()\n",
        "naives_bayes.fit(training_data, y_train)\n",
        "naives_bayes.fit(testing_data, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEQQ5nyz_ZMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Realizamos predicción con el conjunto de pruebas\n",
        "predicciones=naives_bayes.predict(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz03FS0sD7bn",
        "colab_type": "text"
      },
      "source": [
        "### Evaluación del modelo\n",
        "\n",
        "Para todas las mediciones - cuyo rango de probabilidad es entre 0 y 1 - tener una puntuación cercana al valor 1 es buen estimador del comportamiento del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZGFCTGW_ZI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# El accuracy informa cómo el clasificador realiza la predicción correcta de tweets\n",
        "print(\"Accuracy: \",format(accuracy_score(y_test,predicciones)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyyr9bI2_ZGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# La precisión informa la proporción de tweets que clasificamos como verdaderos\n",
        "print(\"Métrica de Precisión del Modelo: \",format(precision_score(y_test,predicciones)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G55sz_gJ_ZEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# La confianza o recall, da la proporción que realmente era verdadero y fueron clasificados como tal.\n",
        "print(\"Confianza o recall del Modelo: \",format(f1_score(y_test, predicciones)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcaFc6JTaj9Z",
        "colab_type": "text"
      },
      "source": [
        "# Regresion Logistica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdxaORxO7RO",
        "colab_type": "text"
      },
      "source": [
        "######   **Modelo 0**: Simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79p7sasOO7RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorizer, simple \n",
        "vectorizer=CountVectorizer()\n",
        "vectorizer.fit(train_text)\n",
        "X_train=vectorizer.transform(train_text)\n",
        "X_test=vectorizer.transform(test_text)\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6N9OteO7RX",
        "colab_type": "text"
      },
      "source": [
        "###### *Modelo 1*: Agregando stopwords y eliminando caracteres tiles y ese tipo de cosas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FbAMTu3O7RY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorizer1, agregando stopwords y distintos n_grams\n",
        "stop_words=stopwords.words('english')\n",
        "vectorizer1=CountVectorizer(strip_accents='ascii',stop_words=stop_words,ngram_range=(1,5),max_df=0.95)\n",
        "vectorizer1.fit(train_text)\n",
        "X_train1=vectorizer1.transform(train_text)\n",
        "X_test1=vectorizer1.transform(test_text)\n",
        "X_train1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3X5rmD3O7Rf",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 2: Uso de TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UauxkUhOO7Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorizer2, usando TF-IDF\n",
        "vectorizer2=TfidfVectorizer()\n",
        "vectorizer2.fit(train_text)\n",
        "X_train2=vectorizer2.transform(train_text)\n",
        "X_test2=vectorizer2.transform(test_text)\n",
        "X_train2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdt5jPC9O7Ro",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 3: Uso de TF-IDF,sacando acentos y analizando varios n_grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R3vhrcyO7Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorizer3, usando TF-iDF, sacando acentos y agreando n_grams\n",
        "max_n3=15\n",
        "vectorizer3=TfidfVectorizer(strip_accents='ascii',ngram_range=(1,max_n3),)\n",
        "vectorizer3.fit(train_text)\n",
        "X_train3=vectorizer3.transform(train_text)\n",
        "X_test3=vectorizer3.transform(test_text)\n",
        "X_train3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeRmWwc_O7Ry",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 4: Inclusion de Lemmatizador, NO incluye uso de TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjmOKMBzO7Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorizer4\n",
        "# Creamos un lemmatizador \n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "vectorizer4=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer4.fit(train_text)\n",
        "X_train4=vectorizer4.transform(train_text)\n",
        "X_test4=vectorizer4.transform(test_text)\n",
        "X_train4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2w0mjiO7R5",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 5( mejores resultados):  \n",
        "- Uso de columna keyword + text\n",
        "- Lemmatizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqzscyKDO7R6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text_y_keyword=train_na['keyword+text']\n",
        "test_text_y_keyword=test_na['keyword+text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTcDe22cO7SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizo keyword en mi analisis\n",
        "vectorizer5=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer5.fit(train_text_y_keyword)\n",
        "X_train5=vectorizer5.transform(train_text_y_keyword)\n",
        "X_test5=vectorizer5.transform(test_text_y_keyword)\n",
        "X_train5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MgB1RI-O7SI",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 6: Utilizo keyword en mi analisis y TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd3H5dUTO7SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizo keyword en mi analisis y TF-IDF\n",
        "vectorizer6=TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer6.fit(train_text_y_keyword)\n",
        "X_train6=vectorizer6.transform(train_text_y_keyword)\n",
        "X_test6=vectorizer6.transform(test_text_y_keyword)\n",
        "X_train6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwID8NvzO7SO",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 7: Inclusion de columna location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx5M--hKO7SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agregamos columna location \n",
        "train_text_keyword_location=train_na.text+''+train_na.keyword+''+train_na.location\n",
        "test_text_keyword_location=test_na.text+''+test_na.keyword+''+test_na.location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPtdyA7ZO7SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer7=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer7.fit(train_text_keyword_location)\n",
        "X_train7=vectorizer7.transform(train_text_keyword_location)\n",
        "X_test7=vectorizer7.transform(test_text_keyword_location)\n",
        "X_train7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajSBLrnzO7Se",
        "colab_type": "text"
      },
      "source": [
        "###### Modelo 8: Utilizamos funcion train_test_split.....\n",
        "- No tiene ningun tipo de mejora al modelo, es solo para probar, se puede utlizar para calcular el Score en lugar de hacer sub\n",
        "- Despues se generaliza para todo el set de entrenamiento y se predice set de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uoi67poO7Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_na['keyword+text']\n",
        "y=train_na['target']\n",
        "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3g3xgmzO7Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer8=CountVectorizer(strip_accents='ascii',tokenizer=LemmaTokenizer())\n",
        "vectorizer8.fit(X_train_)\n",
        "X_train8=vectorizer8.transform(X_train_)\n",
        "X_test8=vectorizer8.transform(X_test_)\n",
        "X_train8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nug83Y-CO7Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Calculamos Score con este metodo, sin hacer cross validation\n",
        "cls.fit(X_train8,y_train_)\n",
        "prediction0=cls.predict(X_test8)\n",
        "print(f1_score(y_test_,prediction0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9FYiV7O7UB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RC=RidgeClassifier()\n",
        "RC.fit(X_train8,y_train_)\n",
        "RC_predict=RC.predict(X_test8)\n",
        "f1_score(RC_predict,y_test_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqOQp9aeO7Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Progabmos el score con algunos de los clasificadores\n",
        "gbc.fit(X_train8,y_train_)\n",
        "cls.fit(X_train8,y_train_)\n",
        "prediction0=cls.predict(X_test8)\n",
        "prediction1=gbc.predict(X_test8)\n",
        "print('RL: {}'.format(f1_score(y_test_,prediction0)))\n",
        "print('GBC: {}'.format(f1_score(y_test_,prediction1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P87TFsSO7S6",
        "colab_type": "text"
      },
      "source": [
        "## Clasificador propio\n",
        "- Lemmatizador\n",
        "- Cuenta cantidad de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3tyCwppO7S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lemmatizer(BaseEstimator):\n",
        "    def __init__(self):\n",
        "        self.l = WordNetLemmatizer()\n",
        "        \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, x):\n",
        "        x = map(lambda r:  ' '.join([self.l.lemmatize(i.lower()) for i in r.split()]), x)\n",
        "        x = np.array(list(x))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1DeXZnJO7TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length_text(x):\n",
        "    return scaler.fit_transform(np.array(x.str.len()).reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xhBXA9O7TL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm=Lemmatizer()\n",
        "vec=CountVectorizer()\n",
        "lr=LogisticRegression(random_state=0,solver='liblinear')\n",
        "clasificador=Pipeline([\n",
        "    ('features',FeatureUnion([\n",
        "        ('text',Pipeline([\n",
        "            ('lm',lm),\n",
        "            ('vec',vec)])),\n",
        "        ('lenght',Pipeline([\n",
        "            ('car',FunctionTransformer(length_text,validate=False))\n",
        "        ]))\n",
        "    ])),\n",
        "    ('lr',lr)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0PcawPNO7TR",
        "colab_type": "text"
      },
      "source": [
        "- *Cross validation del nuevo clasificador*\n",
        "- *El texto es text+keyword*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3LmFZMdO7TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_val_score(estimator=clasificador,X=train_text_y_keyword,y=train_na.target,cv=6,scoring='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ziyLM_wO7Tc",
        "colab_type": "text"
      },
      "source": [
        "* Calculamos f1 para el nuevo clasificador  (Sigue usando el df spliteado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r36updUhO7Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clasificador.fit(X_train_,y_train_)\n",
        "clas_predict=clasificador.predict(X_test_)\n",
        "f1_score(clas_predict,y_test_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yoikuSlO7Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ahora hacemos que utilice todo el dataset, asi tiene mayor corpus\n",
        "clasificador.fit(train_text_y_keyword,train_na.target)\n",
        "sample_submission['target']=clasificador.predict(test_text_y_keyword)\n",
        "sample_submission.to_csv('sub_9(class).csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGkLIZjrO7Tv",
        "colab_type": "text"
      },
      "source": [
        "* Generalizamos para todo el set de train, asi tiene mayor corpus\n",
        "* Hacemos submit con nuevo clasificador\n",
        "* (No tiene el mejor resultado en kaggle, seria bueno hacer un grid search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlbipRWAY_SQ",
        "colab_type": "text"
      },
      "source": [
        "**Modelo con DF nulos rellenados con otros nombres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCfDjzNY0zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "train_fill=train.fillna(value={'keyword':'nokeyword','location':'nolocation'})\n",
        "test_fill=test.fillna(value={'keyword':'nokeyword','location':'nolocation'})\n",
        "train_fill['text+keyword']=train_fill.text+' '+train_fill.keyword\n",
        "test_fill['text+keyword']=test_fill.text+' '+test_fill.keyword\n",
        "train_fill['Caracteres']=scaler.fit_transform(np.array(train_fill.text.str.len()).reshape(-1,1))\n",
        "test_fill['Caracteres']=scaler.transform(np.array(test_fill.text.str.len()).reshape(-1,1))\n",
        "train_fill.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3TO517uPiZ",
        "colab_type": "text"
      },
      "source": [
        "###  Generacion de nuevo DF para el train que incluye la cantidad de caracteres como variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OgS0X7fY1HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer9=TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
        "X_train9=vectorizer9.fit_transform(train_fill['text+keyword'])\n",
        "X_test9=vectorizer9.transform(test_fill['text+keyword'])\n",
        "X_train9\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF2Bbg4rY1Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XX_train=pd.concat([train_fill['Caracteres'].to_frame(),pd.DataFrame(X_train9.toarray())],axis=1)\n",
        "YY_train=train_fill.target\n",
        "XX_test=pd.concat([test_fill['Caracteres'].to_frame(),pd.DataFrame(X_test9.toarray())],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEXHM_qcY1i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cls.fit(XX_train,YY_train)\n",
        "predict=cls.predict(XX_test)\n",
        "sample_submission['target']=predict\n",
        "from google.colab import files\n",
        "sample_submission.to_csv(\"sub_10.csv\",index=False)\n",
        "files.download('sub_10.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytF8YvZ8jDVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "XX_train_, XX_test_, YY_train_, YY_test_ = train_test_split(XX_train, YY_train, test_size=0.20, random_state=0)\n",
        "params={'C':np.logspace(-3,3,7),'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']}\n",
        "cls=LogisticRegression()\n",
        "grid=GridSearchCV(cls,params,cv=8,scoring='f1')\n",
        "grid.fit(XX_train_,YY_train_)\n",
        "print('parametros tuneados con grid search: {}'.format(grid.best_params_))\n",
        "print('f1: ',grid.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_biyNyuBosB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b7y3hx-ghoL",
        "colab_type": "text"
      },
      "source": [
        "# Redes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79aFIvAFDgrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Hacemos el test_train metodo\n",
        "XX_train_, XX_test_, YY_train_, YY_test_ = train_test_split(XX_train, YY_train, test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpD1s7Z971Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim=XX_train_.shape[1]\n",
        "modelito=Sequential()\n",
        "modelito.add(layers.Dense(5,input_dim=dim,activation='relu'))\n",
        "modelito.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TramcaQK8pOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelito.compile(loss='binary_crossentropy',\\\n",
        "                 optimizer='adam',\\\n",
        "                 metrics=['accuracy'])\n",
        "modelito.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrT0RLl1_gW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=modelito.fit(XX_train_,YY_train_,\\\n",
        "                    epochs=100,verbose=False,\\\n",
        "                    validation_data=(XX_test_,YY_test_),batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN72jrGZEMCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelito.evaluate(XX_test_,YY_test_,verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvohq0caHVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=modelito.predict(XX_test_)\n",
        "pred_bool=np.round(pred).astype(int)\n",
        "print(classification_report(YY_test_,pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB1lbBqw49kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    from keras import backend as K\n",
        "### Definimos metricas para generar f1\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeGw0BvO3_gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZAaUbuGz0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Generalizacion\n",
        "dim1=XX_train.shape[1]\n",
        "red1=Sequential()\n",
        "red1.add(layers.Dense(5,input_dim=dim,activation='relu'))\n",
        "red1.add(layers.Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjnBgKQTH6Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red1.compile(loss='binary_crossentropy',\\\n",
        "                 optimizer='adam',\\\n",
        "                 metrics=['accuracy'])\n",
        "red1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ5MIIBpIAgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets1=red1.fit(XX_train,YY_train,\\\n",
        "                    epochs=100,verbose=False,\\\n",
        "                    batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egwoNexqIhuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_red1=red1.predict(XX_test)\n",
        "pred_red1=np.round(pred_red1).astype(int)\n",
        "sample_submission['target']=pred_red1\n",
        "sample_submission.to_csv('sub11(RN).csv',index=False)\n",
        "from google.colab import files\n",
        "files.download('sub11(RN).csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-hfhOHy4k8o",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTFG47lXIFVU",
        "colab_type": "text"
      },
      "source": [
        "DF para hacer Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFSpyAtxIEGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['text+keyword'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kZBdeDdpGG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJnkDARSa9vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sRoXk9t89kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token=Tokenizer(num_words=5000)\n",
        "token.fit_on_texts(X_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhX6mAYD9ib3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=token.texts_to_sequences(X_train)\n",
        "X_emb_test=token.texts_to_sequences(X_test)\n",
        "vocab_size=len(token.word_index)+1\n",
        "maxlen=100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_S0ggp-bhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=pad_sequences(X_emb_train,padding='post',maxlen=maxlen)\n",
        "X_emb_test=pad_sequences(X_emb_test,padding='post',maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jvyAuJO-kWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_emb_train[0,:].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpgW6I1m_kZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2=Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlQKVLqkAGFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wdims=50\n",
        "model2.add(layers.Embedding(input_dim=vocab_size,input_length=100,output_dim=dims))\n",
        "model2.add(layers.GlobalMaxPool1D())\n",
        "model2.add(layers.Dense(10, activation='relu'))\n",
        "model2.add(layers.Dense(1, activation='sigmoid'))\n",
        "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69kmpgC8Ayzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_emb=model2.fit(X_emb_train,y_train,epochs=20,verbose=2,validation_data=(X_emb_test,y_test),\\\n",
        "                     batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNWp2IGNNDwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graf_10210=plot_history(tweets_emb)   #### 10 capas densas, 2 epochs y 10 de batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6cP1BATUvqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graf_52032=plot_history(tweets_emb)  ###5 capas densas, 20 epochs y 32 de batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIiuIjLxNJVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict(X_emb_test)\n",
        "pred_bool=np.round(pred).astype(int)\n",
        "print(classification_report(y_test,pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zft1eONEReoa",
        "colab_type": "text"
      },
      "source": [
        "Generalizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF0yiHRjNlOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token1=Tokenizer(num_words=5000)\n",
        "token1.fit_on_texts(train_fill['text+keyword'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKg0eH72dDF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=token1.texts_to_sequences(train_fill['text+keyword'])\n",
        "X_emb_test=token1.texts_to_sequences(test_fill['text+keyword'])\n",
        "vocab_size=len(token1.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzjZ1xSwdPyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train=pad_sequences(X_emb_train,padding='post',maxlen=100)\n",
        "X_emb_test=pad_sequences(X_emb_test,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoneZp7NddFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub12=Sequential()\n",
        "dims=50\n",
        "sub12.add(layers.Embedding(input_dim=vocab_size,input_length=100,output_dim=dims))\n",
        "sub12.add(layers.GlobalMaxPool1D())\n",
        "sub12.add(layers.Dense(10, activation='relu'))\n",
        "sub12.add(layers.Dense(1, activation='sigmoid'))\n",
        "sub12.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "sub12.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1RNgF-QdzdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_emb=sub12.fit(X_emb_train,train_fill['target'],epochs=2,verbose=2,\\\n",
        "                     batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhcLQNBbeGc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_emb=sub12.predict(X_emb_test)\n",
        "pred_emb=np.round(pred_emb).astype(int)\n",
        "sample_submission['target']=pred_emb\n",
        "sample_submission.to_csv('sub12(EMB).csv',index=False)\n",
        "files.download('sub12(EMB).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUEhZrgne7cF",
        "colab_type": "text"
      },
      "source": [
        "**Embedding pre-entrenado**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mykbd_o09l8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['text+keyword'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe3OWhO2xtth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOl5hKkPfc3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  \n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomtWg_WnEaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(r'glove.6B.100d.txt',token1.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2dg22z68_Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(token1.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jZ0usNw-Lld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_gl= Sequential()\n",
        "model_gl.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=embedding_dim, \n",
        "                           trainable=True))\n",
        "model_gl.add(layers.GlobalMaxPool1D())\n",
        "model_gl.add(layers.Dense(10, activation='relu'))\n",
        "model_gl.add(layers.Dense(1, activation='sigmoid'))\n",
        "model_gl.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_gl.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vj7qaHZ_rKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_gl=model_gl.fit(X_emb_train,y_train,epochs=5,verbose=2,validation_data=(X_emb_test,y_test),\\\n",
        "                     batch_size=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDXjT_iAGIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tweets_gl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgeUj6bN0Fw",
        "colab_type": "text"
      },
      "source": [
        "Generalizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSd2zh-DEDpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token1=Tokenizer(num_words=5000)\n",
        "token1.fit_on_texts(train_fill['text+keyword'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Cmvh2HN1RA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train_1=token1.texts_to_sequences(train_fill['text+keyword'])\n",
        "X_emb_test_1=token1.texts_to_sequences(test_fill['text+keyword'])\n",
        "vocab_size=len(token1.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmf3FhtNOcSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_train_1=pad_sequences(X_emb_train_1,padding='post',maxlen=100)\n",
        "X_emb_test_1=pad_sequences(X_emb_test_1,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8IikAi2Oif2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_1 = create_embedding_matrix(r'glove.6B.100d.txt',token1.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9qs4QwOuFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(token1.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "nonzero_elements / vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urRd5t1BOyjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub13= Sequential()\n",
        "sub13.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=50, \n",
        "                           trainable=True))\n",
        "sub13.add(layers.GlobalMaxPool1D())\n",
        "sub13.add(layers.Dense(10, activation='relu'))\n",
        "sub13.add(layers.Dense(1, activation='sigmoid'))\n",
        "sub13.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub13.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iPtKwrEO_sN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_gl=sub13.fit(X_emb_train,train_fill['target'],epochs=5,verbose=2,batch_size=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2U2-MyjPgqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_gl=sub13.predict(X_emb_test)\n",
        "pred_gl=np.round(pred_gl).astype(int)\n",
        "sample_submission['target']=pred_gl\n",
        "sample_submission.to_csv('sub13(GLOVE).csv',index=False)\n",
        "files.download('sub13(GLOVE).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1JbunlYJ19G",
        "colab_type": "text"
      },
      "source": [
        "## CONV1d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3MMbZbVJ7j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim=100\n",
        "conv=Sequential()\n",
        "conv.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix], trainable=True))\n",
        "conv.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "conv.add(layers.GlobalMaxPooling1D())\n",
        "conv.add(layers.Dense(10, activation='relu'))\n",
        "conv.add(layers.Dense(1, activation='sigmoid'))\n",
        "conv.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "conv.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TS8SnKI6aH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tw_conv=conv.fit(X_emb_train,y_train,epochs=10,verbose=2,\n",
        "                 validation_data=(X_emb_test,y_test),batch_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GQCz3-c6yEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(tw_conv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgOfUuVikBJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def create_model(init_mode='he_uniform'):\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix], trainable=False))\n",
        "    model.add(layers.Conv1D(num_filters,kernel_size, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXh5zV54nQbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "model_CV = KerasClassifier(build_fn=create_model, epochs=epochs, \n",
        "                           batch_size=batch_size, verbose=2)\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "\n",
        "param_grid = dict(init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_emb_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJpRLIwMyWin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEEViqxm3wFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "def create_model_1(num_filters,kernel_size,vocab_size,embedding_dim,maxlen,init_mode='he_uniform'):\n",
        "\n",
        "    model=Sequential()\n",
        "    model.add(layers.Embedding(vocab_size,emb_dim,input_length=maxlen,weights=[embedding_matrix], trainable=True))\n",
        "    model.add(layers.Conv1D(num_filters,kernel_size, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "    model.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TZ5Iz6y2k3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tuneo de hiperparametros\n",
        "%%time\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "param_grid_1= dict(num_filters=[32, 64, 128],kernel_size=[3, 5, 7],\n",
        "                  vocab_size=[vocab_size],embedding_dim=[embedding_dim],\n",
        "                  maxlen=[maxlen],epochs=[10,20],\n",
        "                  batch_size=[10,32,64])\n",
        "model_1 = KerasClassifier(build_fn=create_model_1, verbose=2)\n",
        "grid_1= RandomizedSearchCV(estimator=model_1, param_distributions=param_grid_1,\n",
        "                           cv=4, verbose=2, n_iter=5,n_jobs=-1,scoring='f1')\n",
        "grid_result_1 = grid_1.fit(X_emb_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh-oXIijplBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parametros=grid_result_1.best_params_\n",
        "parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAcBO7OxrT-j",
        "colab_type": "text"
      },
      "source": [
        "**Generalizacion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3WWIccDrXpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_14=Sequential()\n",
        "sub_14.add(layers.Embedding(vocab_size,emb_dim,input_length=100,weights=[embedding_matrix_1], trainable=True))\n",
        "sub_14.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_14.add(layers.GlobalMaxPooling1D())\n",
        "sub_14.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_14.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_14.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_14.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws7szkxIvMFt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgqesInlstMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub14=sub_14.fit(X_emb_train_1,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7zfS-4RvoYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_14=sub_14.predict(X_emb_test_1)\n",
        "pred_sub_14=np.round(pred_sub_14).astype(int)\n",
        "sample_submission['target']=pred_sub_14\n",
        "sample_submission.to_csv('sub14(GLOVE_tun).csv',index=False)\n",
        "files.download('sub14(GLOVE_tun).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-CNyIHByRe",
        "colab_type": "text"
      },
      "source": [
        "## Trabajo con dataset \"limpio\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhyatYcDJuRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleanr = re.compile('https?://\\S+|www\\.\\S+')\n",
        "p=[]\n",
        "for i in range(len(train_fill)):\n",
        "    p.append(re.sub(cleanr, '', train_fill['text+keyword'].loc[i]))\n",
        "train_fill['t&k_sin_url']=p\n",
        "p1=[]\n",
        "for i in range(len(test_fill)):\n",
        "    p1.append(re.sub(cleanr, '', test_fill['text+keyword'].loc[i]))\n",
        "test_fill['t&k_sin_url']=p1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D04EYc1oB3xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "st = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "train_fill['t&k_sin_url']=train_fill['t&k_sin_url'].apply(lambda x: clean_text(x))\n",
        "test_fill['t&k_sin_url']=test_fill['t&k_sin_url'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcUNrj5rSukx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=train_fill['t&k_sin_url']\n",
        "y=np.asarray(train_fill.target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ne4OPZDBP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=[]\n",
        "x=train_fill['t&k_sin_url']\n",
        "for i in range(len(x)):\n",
        "    a.append(TextBlob(train_fill.loc[i,'t&k_sin_url']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1-gaCWyHWE-",
        "colab": {}
      },
      "source": [
        "token2=Tokenizer(num_words=5000)\n",
        "token2.fit_on_texts(train_fill['t&k_sin_url'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lC5aQs9MHWFb",
        "colab": {}
      },
      "source": [
        "X_emb_train_2=token2.texts_to_sequences(train_fill['t&k_sin_url'])\n",
        "X_emb_test_2=token2.texts_to_sequences(test_fill['t&k_sin_url'])\n",
        "vocab_size_2=len(token2.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZSTtoppHWFq",
        "colab": {}
      },
      "source": [
        "X_emb_train_2=pad_sequences(X_emb_train_2,padding='post',maxlen=100)\n",
        "X_emb_test_2=pad_sequences(X_emb_test_2,padding='post',maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6ufjafPHWF3",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_2 = create_embedding_matrix(r'glove.6B.100d.txt',token2.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yWDgQ93eHWGF",
        "colab": {}
      },
      "source": [
        "vocab_size_2=len(token2.word_index)+1\n",
        "nonzero_elements_2 = np.count_nonzero(np.count_nonzero(embedding_matrix_2, axis=1))\n",
        "nonzero_elements_2/ vocab_size_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ALraQP31KEuk",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_15=Sequential()\n",
        "sub_15.add(layers.Embedding(vocab_size_2,embedding_dim,input_length=100,weights=[embedding_matrix_2], trainable=False))\n",
        "sub_15.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_15.add(layers.GlobalMaxPooling1D())\n",
        "sub_15.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_15.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_15.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_15.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pfYYhgwtKEvA",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub15=sub_15.fit(X_emb_train_2,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zUgncKCK_Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_15=sub_15.predict(X_emb_test_2)\n",
        "pred_sub_15=np.round(pred_sub_15).astype(int)\n",
        "sample_submission['target']=pred_sub_15\n",
        "sample_submission.to_csv('sub15(GLOVE_tun(t=false)).csv',index=False)\n",
        "files.download('sub15(GLOVE_tun(t=false)).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxF6N01PMRBc",
        "colab_type": "text"
      },
      "source": [
        "Mismo modelo pero con trainable=true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyK7fBIkMP4j",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "sub_16=Sequential()\n",
        "sub_16.add(layers.Embedding(vocab_size_2,embedding_dim,input_length=100,weights=[embedding_matrix_2], trainable=True))\n",
        "sub_16.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "sub_16.add(layers.GlobalMaxPooling1D())\n",
        "sub_16.add(layers.Dense(10, activation='relu',kernel_initializer=init_mode))\n",
        "sub_16.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "sub_16.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "sub_16.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PGlcsFuwMP47",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub16=sub_16.fit(X_emb_train_2,train_fill['target'],epochs=20,verbose=2,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f8jc0qQ5MP5O",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "pred_sub_16=sub_16.predict(X_emb_test_2)\n",
        "pred_sub_16=np.round(pred_sub_16).astype(int)\n",
        "sample_submission['target']=pred_sub_16\n",
        "sample_submission.to_csv('sub16(GLOVE_tun(t=true)).csv',index=False)\n",
        "files.download('sub16(GLOVE_tun(t=true)).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZDas6LMvoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(train_fill['t&k_sin_url'],train_fill['target'], test_size=0.20, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mRT_ToUju-Ui",
        "colab": {}
      },
      "source": [
        "token3=Tokenizer(num_words=10000)\n",
        "token3.fit_on_texts(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xi5JCHGOu-VM",
        "colab": {}
      },
      "source": [
        "X_emb_train_3=token3.texts_to_sequences(X)\n",
        "X_emb_test_3=token3.texts_to_sequences(test_fill['t&k_sin_url'])\n",
        "vocab_size_3=len(token3.word_index)+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L4imDOtku-Vq",
        "colab": {}
      },
      "source": [
        "maxlen=32\n",
        "X_emb_train_3=pad_sequences(X_emb_train_3,padding='post',maxlen=maxlen)\n",
        "X_emb_test_3=pad_sequences(X_emb_test_3,padding='post',maxlen=maxlen)\n",
        "indices = np.arange(X_emb_train_3.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X_emb_train_3= X_emb_train_3[indices]\n",
        "y=y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkihCitUOJyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_emb_test_3[700]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcedHfs3u-XZ",
        "colab": {}
      },
      "source": [
        "init_mode='he_uniform'\n",
        "dims=50\n",
        "modelito=Sequential()\n",
        "modelito.add(layers.Embedding(input_dim=vocab_size_3,input_length=maxlen,output_dim=dims))\n",
        "modelito.add(layers.Conv1D(128,kernel_size=7, activation='relu',kernel_initializer=init_mode))\n",
        "modelito.add(layers.GlobalMaxPooling1D())\n",
        "modelito.add(layers.Dense(32,activation='relu',kernel_initializer=init_mode))\n",
        "modelito.add(layers.Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n",
        "modelito.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "modelito.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "egPGOf7su-Xz",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "tw_sub_modelito=modelito.fit(X_emb_train_3,y,epochs=10,verbose=2,batch_size=10,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_aNR2vScM6jB",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix_3 = create_embedding_matrix(r'glove.6B.100d.txt',token3.word_index, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NymxGp7jM6jN",
        "colab": {}
      },
      "source": [
        "vocab_size_3=len(token3.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix_3, axis=1))\n",
        "nonzero_elements / vocab_size_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5fY8dzCTEQ7",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size_3,output_dim=25, input_length=maxlen))\n",
        "model.add(layers.Conv1D(32,kernel_size=5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D())\n",
        "model.add(layers.Conv1D(32, kernel_size=5,activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR5YTLDxaTSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc',f1_m])\n",
        "history = model.fit(X_emb_train_3,y,epochs=50,batch_size=10,validation_split=0.2,verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNoyFi7wchtL",
        "colab_type": "text"
      },
      "source": [
        "Modelo mas basico, usando **BOW** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZHxW8DvcgFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe=Pipeline([('vec',CountVectorizer()),\n",
        "               ('cls',LogisticRegression())])\n",
        "modelo=pipe.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roCF9QaDy0Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediccion=model_selection.cross_val_score(modelo,X,y,cv=6,scoring='f1')\n",
        "prediccion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQUVR-BLd0Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                 ('tfidf', TfidfTransformer()),\n",
        "                 ('model', DecisionTreeClassifier(criterion= 'entropy', max_depth = 20, splitter='best', random_state=0))])\n",
        "modelo= pipe.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMSUJ1MOfc3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediccion=model_selection.cross_val_score(modelo,X,y,cv=6,scoring='f1')\n",
        "prediccion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4s6PSdWhRRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer5=CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "vectorizer5.fit(X)\n",
        "X_train5=vectorizer5.transform(X)\n",
        "X_test5=vectorizer5.transform(test_fill['t&k_sin_url'])\n",
        "X_train5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ZsByFgjlDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=xgb.XGBClassifier(random_state=1)\n",
        "model.fit(X_train5,y)\n",
        "model_selection.cross_val_score(model,X_train5,y,scoring='f1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkvxOyyLkAnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "sample_submission['target']=pred\n",
        "sample_submission.to_csv('sub17(BOW_limpio).csv',index=False)\n",
        "files.download('sub17(BOW_limpio).csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}